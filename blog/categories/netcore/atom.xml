<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Netcore | My Octopress Blog]]></title>
  <link href="http://martinbj2008.github.io/blog/categories/netcore/atom.xml" rel="self"/>
  <link href="http://martinbj2008.github.io/"/>
  <updated>2015-05-21T16:26:25+08:00</updated>
  <id>http://martinbj2008.github.io/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Netdevice Watchdog Cause Tx Queue Schedule]]></title>
    <link href="http://martinbj2008.github.io/blog/2014/09/17/netdevice-watchdog-cause-tx-queue-schedule/"/>
    <updated>2014-09-17T15:38:00+08:00</updated>
    <id>http://martinbj2008.github.io/blog/2014/09/17/netdevice-watchdog-cause-tx-queue-schedule</id>
    <content type="html"><![CDATA[<h3>test case</h3>

<p>For ixgbe nic, we want to assign a tx hardware qeueue to each cpu,
and the tx softirq should use the corresponding hardware queue.</p>

<p>each packet will select a softqueue in <code>dev_queue_xmit</code>,
we rewrite ixgbe driver <code>ndo_select_queue</code>(<code>ixgbe_select_queue</code>),
which will return current cpu index(based 0) when packet select queue.
thus for each cpu use its own tx queue.</p>

<p>but, we found some packet had unmatched queue index when send
on specific cpu.</p>

<p>for example, a packet&rsquo;s queue index is 5 but is sent by cpu3,
thus, cpu3 will operate tx hw queue5, which should only be done by cpu5.</p>

<!-- more -->


<h3>Analysis</h3>

<p>When watchdog is start, it first <code>freeze</code> all subqueues,
and the do the check.
At the end, it resume the subqueues,
and reschedule them.</p>

<p>Because the watchdog is handled in a timer,
so the reschedule the queue will be done on a different cpu,
which is different the packets&rsquo;s queue index.</p>

<p>for example:
packet rung select queue on CPU1, while CPU2 run the watchdog,
this packet will be store in the queue1, but not sent.
when cpu2 finish the watchdog, queue1 is rescheduled.
NOTE here the queue1 start run on cpu2 not cpu1.
which is not expected and safe.
it will cause the tx ring buffer hang.</p>

<h3>related source</h3>

<pre><code>232 static void dev_watchdog(unsigned long arg)
233 {
234         struct net_device *dev = (struct net_device *)arg;
235
236         netif_tx_lock(dev);
237         if (!qdisc_tx_is_noop(dev)) {
238                 if (netif_device_present(dev) &amp;&amp;
239                     netif_running(dev) &amp;&amp;
240                     netif_carrier_ok(dev)) {
241                         int some_queue_timedout = 0;
242                         unsigned int i;
243                         unsigned long trans_start;
244
245                         for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
246                                 struct netdev_queue *txq;
247
248                                 txq = netdev_get_tx_queue(dev, i);
249                                 /*
250                                  * old device drivers set dev-&gt;trans_start
251                                  */
252                                 trans_start = txq-&gt;trans_start ? : dev-&gt;trans_start;
253                                 if (netif_xmit_stopped(txq) &amp;&amp;
254                                     time_after(jiffies, (trans_start +
255                                                          dev-&gt;watchdog_timeo))) {
256                                         some_queue_timedout = 1;
257                                         txq-&gt;trans_timeout++;
258                                         break;
259                                 }
260                         }
261
262                         if (some_queue_timedout) {
263                                 WARN_ONCE(1, KERN_INFO "NETDEV WATCHDOG: %s (%s): transmit queue %u timed out\n",
264                                        dev-&gt;name, netdev_drivername(dev), i);
265                                 dev-&gt;netdev_ops-&gt;ndo_tx_timeout(dev);
266                         }
267                         if (!mod_timer(&amp;dev-&gt;watchdog_timer,
268                                        round_jiffies(jiffies +
269                                                      dev-&gt;watchdog_timeo)))
270                                 dev_hold(dev);
271                 }
272         }
273         netif_tx_unlock(dev);
274
275         dev_put(dev);
276 }
</code></pre>

<pre><code>2985 static inline void netif_tx_lock(struct net_device *dev)
2986 {
2987         unsigned int i;
2988         int cpu;
2989
2990         spin_lock(&amp;dev-&gt;tx_global_lock);
2991         cpu = smp_processor_id();
2992         for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
2993                 struct netdev_queue *txq = netdev_get_tx_queue(dev, i);
2994
2995                 /* We are the only thread of execution doing a
2996                  * freeze, but we have to grab the _xmit_lock in
2997                  * order to synchronize with threads which are in
2998                  * the -&gt;hard_start_xmit() handler and already
2999                  * checked the frozen bit.
3000                  */
3001                 __netif_tx_lock(txq, cpu);
3002                 set_bit(__QUEUE_STATE_FROZEN, &amp;txq-&gt;state);
3003                 __netif_tx_unlock(txq);
3004         }
3005 }
</code></pre>

<pre><code>3013 static inline void netif_tx_unlock(struct net_device *dev)
3014 {
3015         unsigned int i;
3016
3017         for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
3018                 struct netdev_queue *txq = netdev_get_tx_queue(dev, i);
3019
3020                 /* No need to grab the _xmit_lock here.  If the
3021                  * queue is not stopped for another reason, we
3022                  * force a schedule.
3023                  */
3024                 clear_bit(__QUEUE_STATE_FROZEN, &amp;txq-&gt;state);
3025                 netif_schedule_queue(txq);
3026         }
3027         spin_unlock(&amp;dev-&gt;tx_global_lock);
3028 }
</code></pre>

<pre><code>2265 static inline void netif_schedule_queue(struct netdev_queue *txq)
2266 {
2267         if (!(txq-&gt;state &amp; QUEUE_STATE_ANY_XOFF))
2268                 __netif_schedule(txq-&gt;qdisc);
2269 }
</code></pre>

<pre><code>2150 static inline void __netif_reschedule(struct Qdisc *q)
2151 {
2152         struct softnet_data *sd;
2153         unsigned long flags;
2154
2155         local_irq_save(flags);
2156         sd = &amp;__get_cpu_var(softnet_data);
2157         q-&gt;next_sched = NULL;
2158         *sd-&gt;output_queue_tailp = q;
2159         sd-&gt;output_queue_tailp = &amp;q-&gt;next_sched;
2160         raise_softirq_irqoff(NET_TX_SOFTIRQ);
2161         local_irq_restore(flags);
2162 }
2163
2164 void __netif_schedule(struct Qdisc *q)
2165 {
2166         if (!test_and_set_bit(__QDISC_STATE_SCHED, &amp;q-&gt;state))
2167                 __netif_reschedule(q);
2168 }
2169 EXPORT_SYMBOL(__netif_schedule);
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcpdump Work With Bonding Interface]]></title>
    <link href="http://martinbj2008.github.io/blog/2014/09/14/tcpdump-work-with-bonding-interface/"/>
    <updated>2014-09-14T08:12:00+08:00</updated>
    <id>http://martinbj2008.github.io/blog/2014/09/14/tcpdump-work-with-bonding-interface</id>
    <content type="html"><![CDATA[<h3>test case</h3>

<h4>On redhat5, Why tcpdump could not work on bonding work.</h4>

<p>OS: redhat 5.
There are two 82599 interfaces eth0 and eth1.
These two interfaces are used as slave of bond0,
eth1 is backup of eth0.</p>

<p>We ping the default gateway on test machine.
ping work OK, and tcpdump on bond0 show the icmp request and icmp require packets.
while on eth0 only icmp request, and eth1 has no any packet.</p>

<!-- more -->


<p>It is impossible there is no incoming packet on any physical interface.
Why tcpdump could not capture the packets on eth0.</p>

<h3>analysis</h3>

<p>tcpdump is <code>pf_socket</code> which is based on <code>ptye_all</code>.</p>

<h4>linux V2.6.32</h4>

<p>In linux v2.6.32, there is bond process before <code>ptye_all</code>,
and thus the <code>skb-&gt;dev</code> will be change to <code>bond0</code> from <code>eth0</code>.
so when packet arrive <code>ptye_all</code>, ony match incoming dev <code>bond0</code>.
we has no chance to capture packet on physical interface eth0.</p>

<h4>upstream linux v3.17-rc4</h4>

<p>bond related process is moved to <code>dev-&gt;rx_handler</code>,
Just like the bridge or openvswitch.</p>

<p>Packet will first be processed by <code>ptype_all</code> with <code>skb-&gt;dev</code> is eth0
and then <code>rx_handler</code>(bond handler for eth0,eth1).
if the rx handler return <code>RX_HANDLER_ANOTHER</code>,
the packet arrive by <code>ptye_all</code> again with different<code>skb-&gt;dev</code> (bond0).</p>

<h4>related patch</h4>

<p><a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5b2c4d">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5b2c4d</a>
<a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=63d8ea">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=63d8ea</a>
<a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5b2c4dd">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5b2c4dd</a></p>

<h5>TODO:</h5>

<p>Test with upstream kernel.</p>

<h3>Redhat source</h3>

<p>redhat source is based on 2.6.32</p>

<pre><code>2616 int __netif_receive_skb(struct sk_buff *skb)
2617 {
...
2636         if (!skb-&gt;iif)
2637                 skb-&gt;iif = skb-&gt;dev-&gt;ifindex;
2638 
2639         /*
2640          * bonding note: skbs received on inactive slaves should only
2641          * be delivered to pkt handlers that are exact matches.  Also
2642          * the deliver_no_wcard flag will be set.  If packet handlers
2643          * are sensitive to duplicate packets these skbs will need to
2644          * be dropped at the handler.  The vlan accel path may have
2645          * already set the deliver_no_wcard flag.
2646          */
2647 
2648         null_or_orig = NULL;
2649         orig_dev = skb-&gt;dev;
2650         if (skb-&gt;deliver_no_wcard)
2651                 null_or_orig = orig_dev;
2652         else if (orig_dev-&gt;master) {
2653                 if (skb_bond_should_drop(skb)) {
2654                         skb-&gt;deliver_no_wcard = 1;
2655                         null_or_orig = orig_dev; /* deliver only exact match */
2656                 } else
2657                         skb-&gt;dev = orig_dev-&gt;master;
2658         }
...
2677         list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {
2678                 if (ptype-&gt;dev == null_or_orig || ptype-&gt;dev == skb-&gt;dev ||
2679                     ptype-&gt;dev == orig_dev) {
2680                         if (pt_prev)
2681                                 ret = deliver_skb(skb, pt_prev, orig_dev);
2682                         pt_prev = ptype;
2683                 }
2684         }
</code></pre>

<h4>upstream linux V3.16</h4>

<pre><code>3579 static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
3580 {
...
3604 another_round:
...
3626         list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {
3627                 if (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) {
3628                         if (pt_prev)
3629                                 ret = deliver_skb(skb, pt_prev, orig_dev);
3630                         pt_prev = ptype;
3631                 }
3632         }
3633
3634 skip_taps:
3635 #ifdef CONFIG_NET_CLS_ACT
3636         skb = handle_ing(skb, &amp;pt_prev, &amp;ret, orig_dev);
3637         if (!skb)
3638                 goto unlock;
3639 ncls:
3640 #endif
3641
...
3656         rx_handler = rcu_dereference(skb-&gt;dev-&gt;rx_handler);
3657         if (rx_handler) {
3658                 if (pt_prev) {
3659                         ret = deliver_skb(skb, pt_prev, orig_dev);
3660                         pt_prev = NULL;
3661                 }
3662                 switch (rx_handler(&amp;skb)) {
3663                 case RX_HANDLER_CONSUMED:
3664                         ret = NET_RX_SUCCESS;
3665                         goto unlock;
3666                 case RX_HANDLER_ANOTHER:
3667                         goto another_round;
3668                 case RX_HANDLER_EXACT:
3669                         deliver_exact = true;
3670                 case RX_HANDLER_PASS:
3671                         break;
3672                 default:
3673                         BUG();
3674                 }
3675         }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Qdisc Running Flag]]></title>
    <link href="http://martinbj2008.github.io/blog/2014/02/08/qdisc-running-flag/"/>
    <updated>2014-02-08T11:26:00+08:00</updated>
    <id>http://martinbj2008.github.io/blog/2014/02/08/qdisc-running-flag</id>
    <content type="html"><![CDATA[<h3>Summary</h3>

<p>In <code>struct Qdisc</code>, there are two similar fileds.
running flag is stored in <strong><code>__state</code></strong> of <code>struct Qdisc</code>, NOT <strong><code>state</code></strong>.
Every time, when we send a packet from qdisc, the <code>running</code> flag is
set by <code>qdisc_run_begin</code>, and after that, it is removed by <code>qdisc_run_end</code>.</p>

<pre><code class="c"> 84         unsigned long           state;
 ...
 87         unsigned int            __state;
</code></pre>

<h4>todo</h4>

<p> why need busylock?</p>

<!-- more -->


<h4>The values of <code>state</code>.</h4>

<pre><code class="c"> 24 enum qdisc_state_t {
 25         __QDISC_STATE_SCHED,
 26         __QDISC_STATE_DEACTIVATED,
 27         __QDISC_STATE_THROTTLED,
 28 };
</code></pre>

<h4>The value of <code>__state</code>.</h4>

<pre><code class="c"> 33 enum qdisc___state_t {
 34         __QDISC___STATE_RUNNING = 1,
 35 };
</code></pre>

<h2>Running flag</h2>

<h3>check Qdisc running</h3>

<pre><code class="c"> 96 static inline bool qdisc_is_running(const struct Qdisc *qdisc)
 97 {
 98         return (qdisc-&gt;__state &amp; __QDISC___STATE_RUNNING) ? true : false;
 99 }
</code></pre>

<h3>Set Qdisc running</h3>

<pre><code class="c">101 static inline bool qdisc_run_begin(struct Qdisc *qdisc)
102 {
103         if (qdisc_is_running(qdisc))
104                 return false;
105         qdisc-&gt;__state |= __QDISC___STATE_RUNNING;
106         return true;
107 }
</code></pre>

<h3>Unset Qdisc running</h3>

<pre><code class="c">109 static inline void qdisc_run_end(struct Qdisc *qdisc)
110 {
111         qdisc-&gt;__state &amp;= ~__QDISC___STATE_RUNNING;
112 }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Xmit a Packet With Qdisc]]></title>
    <link href="http://martinbj2008.github.io/blog/2014/02/08/how-to-xmit-a-packet-with-qdisc/"/>
    <updated>2014-02-08T11:26:00+08:00</updated>
    <id>http://martinbj2008.github.io/blog/2014/02/08/how-to-xmit-a-packet-with-qdisc</id>
    <content type="html"><![CDATA[<h3>summary</h3>

<p>We think it as a ideal and simple case:</p>

<h3>Call Trace:</h3>

<pre><code class="c">&gt; dev_queue_xmit
&gt; &gt;  __dev_queue_xmit(skb, NULL);
&gt; &gt; &gt; rcu_read_lock_bh();
&gt; &gt; &gt; txq = netdev_pick_tx(dev, skb, accel_priv);
&gt; &gt; &gt; q = rcu_dereference_bh(txq-&gt;qdisc);
&gt; &gt; &gt; rc = __dev_xmit_skb(skb, q, dev, txq);
&gt; &gt; &gt; &gt; skb_dst_force(skb);
&gt; &gt; &gt; &gt; q-&gt;enqueue(skb, q);
&gt; &gt; &gt; &gt; qdisc_run_begin(q)
&gt; &gt; &gt; &gt;  __qdisc_run(q);
&gt; &gt; &gt; &gt; &gt; while (qdisc_restart(q))
&gt; &gt; &gt; &gt; &gt; &gt; __netif_schedule
&gt; &gt; &gt; &gt; &gt; qdisc_run_end(q)
&gt; &gt; &gt; rcu_read_unlock_bh();
&gt; &gt; &gt; return rc;
</code></pre>

<!-- more -->


<h3>functions</h3>

<h4><code>__dev_queue_xmit</code></h4>

<pre><code class="c">2806 int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)
2807 {
2808         struct net_device *dev = skb-&gt;dev;
2809         struct netdev_queue *txq;
2810         struct Qdisc *q;
2811         int rc = -ENOMEM;
...
2818         rcu_read_lock_bh();
2819
2820         skb_update_prio(skb);
2821
2822         txq = netdev_pick_tx(dev, skb, accel_priv);
2823         q = rcu_dereference_bh(txq-&gt;qdisc);
2824
2825 #ifdef CONFIG_NET_CLS_ACT
2826         skb-&gt;tc_verd = SET_TC_AT(skb-&gt;tc_verd, AT_EGRESS);
2827 #endif
2828         trace_net_dev_queue(skb);
2829         if (q-&gt;enqueue) {
2830                 rc = __dev_xmit_skb(skb, q, dev, txq);
2831                 goto out;
2832         }
...
2883 out:
2884         rcu_read_unlock_bh();
2885         return rc;
2886 }
</code></pre>

<h3>How to schedlule Qdisc</h3>

<h4>case 1: empty qdisc and qdisc could be bypass</h4>

<p>If the qdisc could be bypass, such as fifo qdisc,
and it is a empty qdisc,
and the qdisc is not running,</p>

<p>set the qdisc as running,
then send the packet directly by <code>sch_direct_xmit</code>.
If send success, clear the running flag by <code>qdisc_run_end</code>,
or(send failed), put the skb to qdisc queue by <code>dev_requeue_skb</code>.</p>

<h4>case 2: enqueue and then send</h4>

<p>In this case, skb must firstly enqueue.
Check and confirm qdisc is running,
if it is not running before check,
call <code>__qdisc_run</code>.</p>

<pre><code class="c">  94 /* qdisc -&gt;enqueue() return codes. */
  95 #define NET_XMIT_SUCCESS        0x00
  96 #define NET_XMIT_DROP           0x01    /* skb dropped                  */
  97 #define NET_XMIT_CN             0x02    /* congestion notification      */
  98 #define NET_XMIT_POLICED        0x03    /* skb is shot by police        */
  99 #define NET_XMIT_MASK           0x0f    /* qdisc flags in net/sch_generic.h */
 100
 101 /* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It
 102  * indicates that the device will soon be dropping packets, or already drops
 103  * some packets of the same priority; prompting us to send less aggressively. */
 104 #define net_xmit_eval(e)        ((e) == NET_XMIT_CN ? 0 : (e))
 105 #define net_xmit_errno(e)       ((e) != NET_XMIT_CN ? -ENOBUFS : 0)
 106
 107 /* Driver transmit return codes */
 108 #define NETDEV_TX_MASK          0xf0
 109
 110 enum netdev_tx {
 111         __NETDEV_TX_MIN  = INT_MIN,     /* make sure enum is signed */
 112         NETDEV_TX_OK     = 0x00,        /* driver took care of packet */
 113         NETDEV_TX_BUSY   = 0x10,        /* driver tx path was busy*/
 114         NETDEV_TX_LOCKED = 0x20,        /* driver tx lock was already taken */
 115 };
 116 typedef enum netdev_tx netdev_tx_t;
</code></pre>

<h4>How <code>__qdisc_run</code> works</h4>

<p><code>__qdisc_run</code> must be embraced by <code>qdisc_run_begin</code> and <code>qdisc_run_end</code>.
Before  <code>__qdisc_run</code>, set flag <code>__QDISC___STATE_RUNNING</code>. after run, remove it.
The flag and two functions ensure a qdisc will run only on a CPU at the smae time.</p>

<pre><code class="c">194 void __qdisc_run(struct Qdisc *q)
195 {
196         int quota = weight_p;
197
198         while (qdisc_restart(q)) {
199                 /*
200                  * Ordered by possible occurrence: Postpone processing if
201                  * 1. we've exceeded packet quota
202                  * 2. another process needs the CPU;
203                  */
204                 if (--quota &lt;= 0 || need_resched()) {
205                         __netif_schedule(q);
206                         break;
207                 }
208         }
209
210         qdisc_run_end(q);
211 }
</code></pre>

<p><code>weight_p</code> is the max count of packets sent in <strong>a</strong> qdisc during a TX softirq.</p>

<p>If the qdisc has little packets, and they will be sent out in the while loop.
Else, the qdisc will be set state as <code>__QDISC_STATE_SCHED</code>,
and the qdisc will linked to <code>output_queue</code> of current cpu&rsquo;s <code>__get_cpu_var(softnet_data)</code>,
the TX softirq will be triggered to sent remain packet in the qdisc.</p>

<pre><code class="c">156 /*
157  * NOTE: Called under qdisc_lock(q) with locally disabled BH.
158  *
159  * __QDISC_STATE_RUNNING guarantees only one CPU can process
160  * this qdisc at a time. qdisc_lock(q) serializes queue accesses for
161  * this queue.
162  *
163  *  netif_tx_lock serializes accesses to device driver.
164  *
165  *  qdisc_lock(q) and netif_tx_lock are mutually exclusive,
166  *  if one is grabbed, another must be free.
167  *
168  * Note, that this procedure can be called by a watchdog timer
169  *
170  * Returns to the caller:
171  *                              0  - queue is empty or throttled.
172  *                              &gt;0 - queue is not empty.
173  *
174  */
175 static inline int qdisc_restart(struct Qdisc *q)
176 {
177         struct netdev_queue *txq;
178         struct net_device *dev;
179         spinlock_t *root_lock;
180         struct sk_buff *skb;
181
182         /* Dequeue packet */
183         skb = dequeue_skb(q);
184         if (unlikely(!skb))
185                 return 0;
186         WARN_ON_ONCE(skb_dst_is_noref(skb));
187         root_lock = qdisc_lock(q);
188         dev = qdisc_dev(q);
189         txq = netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));
190
191         return sch_direct_xmit(skb, q, dev, txq, root_lock);
192 }
</code></pre>

<pre><code class="c">109 /*
110  * Transmit one skb, and handle the return status as required. Holding the
111  * __QDISC_STATE_RUNNING bit guarantees that only one CPU can execute this
112  * function.
113  *
114  * Returns to the caller:
115  *                              0  - queue is empty or throttled.
116  *                              &gt;0 - queue is not empty.
117  */
118 int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,
119                     struct net_device *dev, struct netdev_queue *txq,
120                     spinlock_t *root_lock)
121 {
122         int ret = NETDEV_TX_BUSY;
123
124         /* And release qdisc */
125         spin_unlock(root_lock);
126
127         HARD_TX_LOCK(dev, txq, smp_processor_id());
128         if (!netif_xmit_frozen_or_stopped(txq))
129                 ret = dev_hard_start_xmit(skb, dev, txq);
130
131         HARD_TX_UNLOCK(dev, txq);
132
133         spin_lock(root_lock);
134
135         if (dev_xmit_complete(ret)) {
136                 /* Driver sent out skb successfully or skb was consumed */
137                 ret = qdisc_qlen(q);
138         } else if (ret == NETDEV_TX_LOCKED) {
139                 /* Driver try lock failed */
140                 ret = handle_dev_cpu_collision(skb, txq, q);
141         } else {
142                 /* Driver returned NETDEV_TX_BUSY - requeue skb */
143                 if (unlikely(ret != NETDEV_TX_BUSY))
144                         net_warn_ratelimited("BUG %s code %d qlen %d\n",
145                                              dev-&gt;name, ret, q-&gt;q.qlen);
146
147                 ret = dev_requeue_skb(skb, q);
148         }
149
150         if (ret &amp;&amp; netif_xmit_frozen_or_stopped(txq))
151                 ret = 0;
152
153         return ret;
154 }
</code></pre>

<h4><code>skb-&gt;queue_mapping</code></h4>

<p>In multi-queue nic driver, it is used to indicate which queue is used to xmit packet.
It is set by <code>skb_set_queue_mapping</code> in <code>netdev_pick_tx</code>, <code>__dev_queue_xmit</code>.</p>

<h4>busylock of <code>struct Qdisc</code></h4>

<p>As we said, Qdisc uses <code>__QDISC___STATE_RUNNING</code> to ensure,
for a same qdisc, <strong>ONLY ONE</strong> cpu xmit packet at the same time.
How to manage the other cpus?</p>

<p><code>busylock</code> of <code>struct Qdisc</code> is used for this.
For a same qdisc,
the first CPU, set the <code>__QDISC___STATE_RUNNING</code>.
the second CPU, grab the spinlock <code>busylock</code> of <code>struct Qdisc</code>
for the third or more CPU, wait on spinlock <code>busylock</code> of <code>struct Qdisc</code>.</p>

<h4><code>qdisc_lock</code></h4>

<p>Where is <code>qdisc_lock</code> stored in the Qdisc.
<code>c
255 static inline spinlock_t *qdisc_lock(struct Qdisc *qdisc)
256 {
257         return &amp;qdisc-&gt;q.lock;
258 }
</code></p>

<pre><code class="c">45 struct Qdisc {
...
85         struct sk_buff_head     q;
...
</code></pre>

<pre><code class="c"> 148 struct sk_buff_head {
 149         /* These two members must be first. */
 150         struct sk_buff  *next;
 151         struct sk_buff  *prev;
 152
 153         __u32           qlen;
 154         spinlock_t      lock;
 155 };
</code></pre>

<h4><code>NETIF_F_LLTX</code></h4>

<p><code>HARD_TX_LOCK</code> and <code>HARD_TX_UNLOCK</code> will embrance the driver&rsquo;s <code>ndo_start_xmit</code>.</p>

<pre><code class="c">2805 #define HARD_TX_LOCK(dev, txq, cpu) {                   \
2806         if ((dev-&gt;features &amp; NETIF_F_LLTX) == 0) {      \
2807                 __netif_tx_lock(txq, cpu);              \
2808         }                                               \
2809 }
2810
2811 #define HARD_TX_UNLOCK(dev, txq) {                      \
2812         if ((dev-&gt;features &amp; NETIF_F_LLTX) == 0) {      \
2813                 __netif_tx_unlock(txq);                 \
2814         }                                               \
2815 }
</code></pre>

<p>For physical nic driver, it does nothing.
but for virutal nic device driver, it is used to ensure the packet sent in order.
<code>c
junwei@localhost:~/git/linux$ grep NETIF_F_LLTX net/ -Rw
net/ipv4/ipip.c:        dev-&gt;features           |= NETIF_F_LLTX;
net/ipv4/ip_gre.c:              dev-&gt;features |= NETIF_F_LLTX;
net/ipv4/ip_vti.c:      dev-&gt;features           |= NETIF_F_LLTX;
net/8021q/vlan_dev.c:   dev-&gt;features |= real_dev-&gt;vlan_features | NETIF_F_LLTX;
net/8021q/vlan_dev.c:   features |= NETIF_F_LLTX;
net/bridge/br_device.c: dev-&gt;features = COMMON_FEATURES | NETIF_F_LLTX | NETIF_F_NETNS_LOCAL |
net/l2tp/l2tp_eth.c:    dev-&gt;features           |= NETIF_F_LLTX;
net/ipv6/ip6_gre.c:             dev-&gt;features |= NETIF_F_LLTX;
net/ipv6/ip6_gre.c:             dev-&gt;features |= NETIF_F_LLTX;
net/ipv6/sit.c: dev-&gt;features           |= NETIF_F_LLTX;
net/hsr/hsr_device.c:   hsr_dev-&gt;features |= NETIF_F_LLTX;
net/openvswitch/vport-internal_dev.c:   netdev-&gt;features = NETIF_F_LLTX | NETIF_F_SG | NETIF_F_FRAGLIST |
net/openvswitch/vport-internal_dev.c:   netdev-&gt;hw_features = netdev-&gt;features &amp; ~NETIF_F_LLTX;
</code></p>

<pre><code class="c">2710 static inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)
2711 {
2712         spin_lock(&amp;txq-&gt;_xmit_lock);
2713         txq-&gt;xmit_lock_owner = cpu;
2714 }
</code></pre>

<pre><code class="c">2730 static inline void __netif_tx_unlock(struct netdev_queue *txq)
2731 {
2732         txq-&gt;xmit_lock_owner = -1;
2733         spin_unlock(&amp;txq-&gt;_xmit_lock);
2734 }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Create Dev Qdisc]]></title>
    <link href="http://martinbj2008.github.io/blog/2014/01/28/how-to-create-dev-qdisc/"/>
    <updated>2014-01-28T13:47:00+08:00</updated>
    <id>http://martinbj2008.github.io/blog/2014/01/28/how-to-create-dev-qdisc</id>
    <content type="html"><![CDATA[<h2>Summary</h2>

<h3>Part 1: Register multi queue net device.</h3>

<p>In this part, only the framework is prepared for qdisc,
and the <code>noop_qdisc</code> is set as default.</p>

<h4>prepare <code>netdev_queue</code>s.</h4>

<p>for example: intel igb hardware has 8 hardware tx queue,
and nic driver create 8 corresponding <code>struct netdev_queue</code>
in the <code>_tx</code> of <code>struct net_device</code>.</p>

<h4>prepare <code>mq_qdisc</code></h4>

<p>The <code>mq_qdisc</code> is attached to the corresponding device.
In <code>mq_qdisc</code> private field, a default qdisc will be
create for <strong>each</strong> NIC&rsquo;s hardware queue.
This is done in <code>mq_init</code>.
The default qdisc is <code>pfifo_fast_ops</code>.</p>

<h4>attach <code>mq_qdisc</code> to <code>netdev_queue</code>.</h4>

<p>In <code>mq_attach</code>, these qdiscs are attatched to corresponding
<code>struct netdev_queue</code>.</p>

<h3>Part 2: Active a net device with right qdiscs</h3>

<p>Here only trace with the case <code>mq_qdisc</code>.
When dev is up, <code>dev_open</code> is called, which will call <code>dev_activate</code>.</p>

<!-- more -->


<h4>2.1</h4>

<p>A qdisc with <code>mq_qdisc_ops</code> is alloced and assigned to <code>dev-&gt;qdisc</code>,
The <code>mq qdisc</code> is not a simple qdisc, which includes sub-qdisc in its
private fields. Each sub-qdisc is a default qdisc(pfif0).</p>

<!-- ![mq qdisc](/images/mq/mq.blankflowchar.jpeg) -->


<p><img src="https://www.lucidchart.com/publicSegments/view/52f496a8-5c18-4c12-9d2e-62830a0093bd/image.png" alt="mq qdisc" /></p>

<h4>2.2</h4>

<p>when <code>attach</code> a qdisc to a device by <code>qdisc-&gt;attach</code>(which equal <code>mq_attach</code>),
in <code>mq_attach</code>, each qdisc alloc in 2.1, will be assigned to a
netdevice queue(which is alloced in part1).</p>

<p>by now, each netdevice queue has a qdisc, but the qdisc is assigned to
<code>qdisc_sleeping</code> of <code>netdev_queue</code>.</p>

<p>at last, <code>dev_activate</code> call <code>transition_one_qdisc</code> for each tx queue,
to set <code>netdev_queue-&gt;qdisc</code> with <code>netdev_queue-&gt;qdisc_sleeping</code>.</p>

<p><code>attach_default_qdiscs</code> will be called twice time,
one is called to create with <code>mq_qdisc_ops</code>,
one is called to create with <code>default_qdisc_ops</code>,</p>

<h4>Data Structure</h4>

<h5><code>struct net_device</code> and <code>netdev_queue</code></h5>

<pre><code class="c"> struct net_device {
...
1350         struct netdev_queue     *_tx ____cacheline_aligned_in_smp;
1351
1352         /* Number of TX queues allocated at alloc_netdev_mq() time  */
1353         unsigned int            num_tx_queues;
</code></pre>

<pre><code class="c"> 544 struct netdev_queue {
 ...
 548         struct net_device       *dev;
 549         struct Qdisc            *qdisc;
 550         struct Qdisc            *qdisc_sleeping;
...
 560         spinlock_t              _xmit_lock ____cacheline_aligned_in_smp;
...
 573         unsigned long           state;
</code></pre>

<pre><code class="c"> 21 struct mq_sched {
 22         struct Qdisc            **qdiscs;
 23 };
</code></pre>

<h4>Call trace.</h4>

<h5>Part 1</h5>

<pre><code class="c">&gt; igb_probe
&gt; &gt; alloc_etherdev_mq
&gt; &gt; &gt; alloc_etherdev_mqs
&gt; &gt; &gt; &gt; alloc_netdev_mqs
&gt; &gt; &gt; &gt; &gt; ether_setup
&gt; &gt; &gt; &gt; &gt; netif_alloc_netdev_queues
&gt; &gt; &gt; &gt; &gt; &gt; netdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);
&gt; &gt; &gt; &gt; &gt; &gt; &gt; netdev_init_one_queue
&gt; &gt; register_netdev
&gt; &gt; &gt; register_netdevice
&gt; &gt; &gt; &gt; dev_init_scheduler
&gt; &gt; &gt; &gt; &gt; netdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &amp;noop_qdisc);
&gt; &gt; &gt; &gt; &gt; &gt; dev_init_scheduler_queue
</code></pre>

<p>NOTE:
if disable RPS(google patch), there is no soft queue to receive packet.
because there is <code>igb_ring</code> in the driver.</p>

<h6>Part 2</h6>

<pre><code class="c">&gt; dev_open
&gt; &gt; __dev_open
&gt; &gt; &gt; dev_activate
&gt; &gt; &gt; &gt; attach_default_qdiscs(dev);
&gt; &gt; &gt; &gt; &gt; qdisc_create_dflt(txq, &amp;mq_qdisc_ops, TC_H_ROOT);
&gt; &gt; &gt; &gt; &gt; &gt; qdisc_alloc
&gt; &gt; &gt; &gt; &gt; &gt; ops-&gt;init(equal: mq_init)
&gt; &gt; &gt; &gt; &gt; &gt; &gt; for (ntx = 0; ntx &lt; dev-&gt;num_tx_queues; ntx++) {
&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; qdisc_create_dflt(dev_queue, default_qdisc_ops ...
&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; qdisc_alloc
&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ops-&gt;init
&gt; &gt; &gt; &gt; &gt; qdisc-&gt;ops-&gt;attach
&gt; &gt; &gt; &gt; &gt; dev_graft_qdisc
&gt; &gt; &gt; &gt; netdev_for_each_tx_queue(dev, transition_one_qdisc, &amp;need_watchdog); 
&gt; &gt; &gt; &gt; &gt; transition_one_qdisc
</code></pre>

<h3>Functions in Part 1.</h3>

<pre><code class="c">2016 static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
2017 {
...
2067         netdev = alloc_etherdev_mq(sizeof(struct igb_adapter),
2068                                    IGB_MAX_TX_QUEUES);
2069         if (!netdev)
2070                 goto err_alloc_etherdev;
...
2317         strcpy(netdev-&gt;name, "eth%d");
2318         err = register_netdev(netdev);
2319         if (err)
2320                 goto err_register;
...
</code></pre>

<pre><code class="c"> 51 #define alloc_etherdev_mq(sizeof_priv, count) alloc_etherdev_mqs(sizeof_priv, count, count)
</code></pre>

<pre><code class="c">372 /**
373  * alloc_etherdev_mqs - Allocates and sets up an Ethernet device
374  * @sizeof_priv: Size of additional driver-private structure to be allocated
375  *      for this Ethernet device
376  * @txqs: The number of TX queues this device has.
377  * @rxqs: The number of RX queues this device has.
378  *
379  * Fill in the fields of the device structure with Ethernet-generic
380  * values. Basically does everything except registering the device.
381  *
382  * Constructs a new net device, complete with a private data area of
383  * size (sizeof_priv).  A 32-byte (not bit) alignment is enforced for
384  * this private data area.
385  */
386
387 struct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,
388                                       unsigned int rxqs)
389 {
390         return alloc_netdev_mqs(sizeof_priv, "eth%d", ether_setup, txqs, rxqs);
391 }
392 EXPORT_SYMBOL(alloc_etherdev_mqs);
</code></pre>

<pre><code class="c">6223 /**
6224  *      alloc_netdev_mqs - allocate network device
6225  *      @sizeof_priv:   size of private data to allocate space for
6226  *      @name:          device name format string
6227  *      @setup:         callback to initialize device
6228  *      @txqs:          the number of TX subqueues to allocate
6229  *      @rxqs:          the number of RX subqueues to allocate
6230  *
6231  *      Allocates a struct net_device with private data area for driver use
6232  *      and performs basic initialization.  Also allocates subquue structs
6233  *      for each queue on the device.
6234  */
6235 struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
6236                 void (*setup)(struct net_device *),
6237                 unsigned int txqs, unsigned int rxqs)
6238 {
6239         struct net_device *dev;
6240         size_t alloc_size;
6241         struct net_device *p;
6242
6243         BUG_ON(strlen(name) &gt;= sizeof(dev-&gt;name));
6244
6245         if (txqs &lt; 1) {
6246                 pr_err("alloc_netdev: Unable to allocate device with zero queues\n");
6247                 return NULL;
6248         }
6249
6250 #ifdef CONFIG_RPS
6251         if (rxqs &lt; 1) {
6252                 pr_err("alloc_netdev: Unable to allocate device with zero RX queues\n");
6253                 return NULL;
6254         }
6255 #endif
6256
6257         alloc_size = sizeof(struct net_device);
6258         if (sizeof_priv) {
6259                 /* ensure 32-byte alignment of private area */
6260                 alloc_size = ALIGN(alloc_size, NETDEV_ALIGN);
6261                 alloc_size += sizeof_priv;
6262         }
6263         /* ensure 32-byte alignment of whole construct */
6264         alloc_size += NETDEV_ALIGN - 1;
6265
6266         p = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
6267         if (!p)
6268                 p = vzalloc(alloc_size);
6269         if (!p)
6270                 return NULL;
6271
6272         dev = PTR_ALIGN(p, NETDEV_ALIGN);
6273         dev-&gt;padded = (char *)dev - (char *)p;
6274
6275         dev-&gt;pcpu_refcnt = alloc_percpu(int);
6276         if (!dev-&gt;pcpu_refcnt)
6277                 goto free_dev;
6278
6279         if (dev_addr_init(dev))
6280                 goto free_pcpu;
6281
6282         dev_mc_init(dev);
6283         dev_uc_init(dev);
6284
6285         dev_net_set(dev, &amp;init_net);
6286
6287         dev-&gt;gso_max_size = GSO_MAX_SIZE;
6288         dev-&gt;gso_max_segs = GSO_MAX_SEGS;
6289
6290         INIT_LIST_HEAD(&amp;dev-&gt;napi_list);
6291         INIT_LIST_HEAD(&amp;dev-&gt;unreg_list);
6292         INIT_LIST_HEAD(&amp;dev-&gt;close_list);
6293         INIT_LIST_HEAD(&amp;dev-&gt;link_watch_list);
6294         INIT_LIST_HEAD(&amp;dev-&gt;adj_list.upper);
6295         INIT_LIST_HEAD(&amp;dev-&gt;adj_list.lower);
6296         INIT_LIST_HEAD(&amp;dev-&gt;all_adj_list.upper);
6297         INIT_LIST_HEAD(&amp;dev-&gt;all_adj_list.lower);
6298         dev-&gt;priv_flags = IFF_XMIT_DST_RELEASE;
6299         setup(dev);
6300
6301         dev-&gt;num_tx_queues = txqs;
6302         dev-&gt;real_num_tx_queues = txqs;
6303         if (netif_alloc_netdev_queues(dev))
6304                 goto free_all;
6305
6306 #ifdef CONFIG_RPS
6307         dev-&gt;num_rx_queues = rxqs;
6308         dev-&gt;real_num_rx_queues = rxqs;
6309         if (netif_alloc_rx_queues(dev))
6310                 goto free_all;
6311 #endif
6312
6313         strcpy(dev-&gt;name, name);
6314         dev-&gt;group = INIT_NETDEV_GROUP;
6315         if (!dev-&gt;ethtool_ops)
6316                 dev-&gt;ethtool_ops = &amp;default_ethtool_ops;
6317         return dev;
6318
6319 free_all:
6320         free_netdev(dev);
6321         return NULL;
6322
6323 free_pcpu:
6324         free_percpu(dev-&gt;pcpu_refcnt);
6325         netif_free_tx_queues(dev);
6326 #ifdef CONFIG_RPS
6327         kfree(dev-&gt;_rx);
6328 #endif
6329
6330 free_dev:
6331         netdev_freemem(dev);
6332         return NULL;
6333 }
6334 EXPORT_SYMBOL(alloc_netdev_mqs);
</code></pre>

<pre><code class="c">5742 static int netif_alloc_netdev_queues(struct net_device *dev)
5743 {
5744         unsigned int count = dev-&gt;num_tx_queues;
5745         struct netdev_queue *tx;
5746         size_t sz = count * sizeof(*tx);
5747
5748         BUG_ON(count &lt; 1 || count &gt; 0xffff);
5749
5750         tx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
5751         if (!tx) {
5752                 tx = vzalloc(sz);
5753                 if (!tx)
5754                         return -ENOMEM;
5755         }
5756         dev-&gt;_tx = tx;
5757
5758         netdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);
5759         spin_lock_init(&amp;dev-&gt;tx_global_lock);
5760
5761         return 0;
5762 }
</code></pre>

<pre><code class="c">5720 static void netdev_init_one_queue(struct net_device *dev,
5721                                   struct netdev_queue *queue, void *_unused)
5722 {
5723         /* Initialize queue lock */
5724         spin_lock_init(&amp;queue-&gt;_xmit_lock);
5725         netdev_set_xmit_lockdep_class(&amp;queue-&gt;_xmit_lock, dev-&gt;type);
5726         queue-&gt;xmit_lock_owner = -1;
5727         netdev_queue_numa_node_write(queue, NUMA_NO_NODE);
5728         queue-&gt;dev = dev;
5729 #ifdef CONFIG_BQL
5730         dql_init(&amp;queue-&gt;dql, HZ);
5731 #endif
5732 }
</code></pre>

<pre><code class="c">5972 int register_netdev(struct net_device *dev)
5973 {
5974         int err;
5975
5976         rtnl_lock();
5977         err = register_netdevice(dev);
5978         rtnl_unlock();
5979         return err;
5980 }
5981 EXPORT_SYMBOL(register_netdev);
</code></pre>

<pre><code class="c">5781 int register_netdevice(struct net_device *dev)
5782 {
...
5879         linkwatch_init_dev(dev);
5880
5881         dev_init_scheduler(dev);
5882         dev_hold(dev);
5883         list_netdevice(dev);
5884         add_device_randomness(dev-&gt;dev_addr, dev-&gt;addr_len);
...
</code></pre>

<pre><code class="c">876 void dev_init_scheduler(struct net_device *dev)
877 {
878         dev-&gt;qdisc = &amp;noop_qdisc;
879         netdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &amp;noop_qdisc);
880         if (dev_ingress_queue(dev))
881                 dev_init_scheduler_queue(dev, dev_ingress_queue(dev), &amp;noop_qdisc);
882
883         setup_timer(&amp;dev-&gt;watchdog_timer, dev_watchdog, (unsigned long)dev);
884 }
</code></pre>

<pre><code>866 static void dev_init_scheduler_queue(struct net_device *dev,
867                                      struct netdev_queue *dev_queue,
868                                      void *_qdisc)
869 {
870         struct Qdisc *qdisc = _qdisc;
871
872         dev_queue-&gt;qdisc = qdisc;
873         dev_queue-&gt;qdisc_sleeping = qdisc;
874 }
</code></pre>

<pre><code class="c">366 struct Qdisc noop_qdisc = {
367         .enqueue        =       noop_enqueue,
368         .dequeue        =       noop_dequeue,
369         .flags          =       TCQ_F_BUILTIN,
370         .ops            =       &amp;noop_qdisc_ops,
371         .list           =       LIST_HEAD_INIT(noop_qdisc.list),
372         .q.lock         =       __SPIN_LOCK_UNLOCKED(noop_qdisc.q.lock),
373         .dev_queue      =       &amp;noop_netdev_queue,
374         .busylock       =       __SPIN_LOCK_UNLOCKED(noop_qdisc.busylock),
375 };
</code></pre>

<h4>functions for Part 2.</h4>

<pre><code class="c">745 void dev_activate(struct net_device *dev)
746 {
747         int need_watchdog;
748
749         /* No queueing discipline is attached to device;
750          * create default one for devices, which need queueing
751          * and noqueue_qdisc for virtual interfaces
752          */
753
754         if (dev-&gt;qdisc == &amp;noop_qdisc)
755                 attach_default_qdiscs(dev);
756
757         if (!netif_carrier_ok(dev))
758                 /* Delay activation until next carrier-on event */
759                 return;
760
761         need_watchdog = 0;
762         netdev_for_each_tx_queue(dev, transition_one_qdisc, &amp;need_watchdog);
763         if (dev_ingress_queue(dev))
764                 transition_one_qdisc(dev, dev_ingress_queue(dev), NULL);
765
766         if (need_watchdog) {
767                 dev-&gt;trans_start = jiffies;
768                 dev_watchdog_up(dev);
769         }
770 }
771 EXPORT_SYMBOL(dev_activate);
</code></pre>

<pre><code class="c">708 static void attach_default_qdiscs(struct net_device *dev)
709 {
710         struct netdev_queue *txq;
711         struct Qdisc *qdisc;
712
713         txq = netdev_get_tx_queue(dev, 0);
714
715         if (!netif_is_multiqueue(dev) || dev-&gt;tx_queue_len == 0) {
716                 netdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);
717                 dev-&gt;qdisc = txq-&gt;qdisc_sleeping;
718                 atomic_inc(&amp;dev-&gt;qdisc-&gt;refcnt);
719         } else {
720                 qdisc = qdisc_create_dflt(txq, &amp;mq_qdisc_ops, TC_H_ROOT);
721                 if (qdisc) {
722                         qdisc-&gt;ops-&gt;attach(qdisc);
723                         dev-&gt;qdisc = qdisc;
724                 }
725         }
726 }
</code></pre>

<pre><code class="c">223 static const struct Qdisc_class_ops mq_class_ops = {
224         .select_queue   = mq_select_queue,
225         .graft          = mq_graft,
226         .leaf           = mq_leaf,
227         .get            = mq_get,
228         .put            = mq_put,
229         .walk           = mq_walk,
230         .dump           = mq_dump_class,
231         .dump_stats     = mq_dump_class_stats,
232 };
233
234 struct Qdisc_ops mq_qdisc_ops __read_mostly = {
235         .cl_ops         = &amp;mq_class_ops,
236         .id             = "mq",
237         .priv_size      = sizeof(struct mq_sched),
238         .init           = mq_init,
239         .destroy        = mq_destroy,
240         .attach         = mq_attach,
241         .dump           = mq_dump,
242         .owner          = THIS_MODULE,
243 };
</code></pre>

<pre><code class="c">584 struct Qdisc *qdisc_create_dflt(struct netdev_queue *dev_queue,
585                                 const struct Qdisc_ops *ops,
586                                 unsigned int parentid)
587 {
588         struct Qdisc *sch;
589
590         if (!try_module_get(ops-&gt;owner))
591                 goto errout;
592
593         sch = qdisc_alloc(dev_queue, ops);
594         if (IS_ERR(sch))
595                 goto errout;
596         sch-&gt;parent = parentid;
597
598         if (!ops-&gt;init || ops-&gt;init(sch, NULL) == 0)
599                 return sch;
600
601         qdisc_destroy(sch);
602 errout:
603         return NULL;
604 }
</code></pre>
]]></content>
  </entry>
  
</feed>
